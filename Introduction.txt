# Table of Contents

1. [Challenge Summary] (README.md#challenge-summary)
2. [Details of Implementation] (README.md#details-of-implementation)
3. [Description of Data] (README.md#description-of-data)
4. [Writing clean, scalable and well-tested code](README.md#writing-clean-scalable-and-well-tested-code)
5. [Repo directory structure] (README.md#repo-directory-structure)
6. [Testing your directory structure and output format] (README.md#testing-your-directory-structure-and-output-format)
7. [FAQ] (README.md#faq)

## Hack K-State 2016 : Data Science For Social Good.
Hack K-state kicks off November 11'16 giving 48 hours to anyone interested in coding a chance to display their skills and creativity. The event is meant to bring together engineers, business students, designers, entrepreneurs etc from different disciplines. Starting 2016, Hack Kstate includes Data Science for Social Good(DSSG) track. DSSG uses analysis and visualization of data on topics such as public health, sustainable urban planning, environmental protection, disaster response, crime prevention, education, transportation, governance, commerce, social justice etc. that impact the lives of local community. This is the first ever data science hackathon being organized 

**Why data science?**

Data science is art of deriving meaning from data by understanding how it fits into the larger picture of a society or organization. It utilizes cross discipline skills in Computer Science, business, analytics, statistics, and mathematics.  By 2020 the world will generate 50x the amount of information compared to 2011 [EMC.com]. As per McKinsey Global Institute, The U.S. could face a shortage of up to 190,000 professionals with data science skills by 2018. Data Science highly in demand skill set is applicable to companies in almost all domains of life. This event helps you learn these skills and but also put them directly in use. 

**Theme for 2016 : Healthcare**

This year's DSSG theme is healthcare. Healthcare has slowly been progressing through three waves of data : data collection, data sharing, and data analytics.  Today, there is a huge volume of health data from pharmaceutical, biotecha and healthcare organization. It is a challenge to derive  meaningful value from this data. Healthcare analytics is about  understanding real-world healthcare data and finding a creative solutions that would hav an impact on healthcare. At Hack K-state you will get a chance to do it all, in an intense 48 hour competition.

### Event Details

Hack K-State 2016 kicks off on Friday, November 11, 2016 at 7:30pm with opening presentation at Fiedler Auditorium. The competition officially begins at 9:00 pm. For more details please visit : https://hackkstate.org/schedule/schedule.pdf

We will host two categories under the data science for social good (DSSG) track we Hack K-state:

- **1. Novice:** Are you participating to learn and sharpen your skills at Data Science? This category is a perfect fit for you. We have 3 different problems to chose from:
- **2. Experts:** Regression or Classification, Bayesian or SVM, do you find these terms familiar? This category is perfect for you. If you have some programming skills and have encountered a data problem in past we encourage you to participate under this category. 

##### Challenge #1:  Predicting Hospital Readmissions

In this challenge, your team will try to predict which patients will be re-admitted to the hospital after being discharged from a hospital stay.  This is a real problem for hospitals, who donâ€™t get paid for a re-admission if it happens <30 days after the patient was discharged. You will be given a training data set of several files, and will need to assemble them and train your algorithm.  You are free to use any algorithm you wish that your team codes, or combine several. On Sunday at 9:00 AM, we will release the validation data set. You will run your algorithm and submit your predictions for who got re-admitted from validation data set.  
**Data Description :**

> **Training Set:**  This file (Challenge_1_Training.Set.csv) contains 70,000 HIPAA compliant de-identified records of hospital admissions.  Each record contains a random and one-way scrambled unique identifier, limited demographics (age, gender), type of admission,  discharge disposition (e.g. home, to a skilled nursing facility, home with assistance, transfer to another facility), if the person was re-admitted, and the number of days from the relative to 30 that the re-admission occurred. The (READ_ME.doc) file contains each field definition, as well as additional definitions for Admission Type (admission_type_id.csv), Discharge Disposition (discharge_disposition_id.csv), and Admission Source (admission_source_id.csv)

> **Look-up tables for ICD-9 Diagnosis:**  This zipped files (version 32) from Center for Medicare Services web site (https://www.cms.gov/medicare/coding/ICD9providerdiagnosticcodes/codes.html) contains two tables with the ICD-9 Diagnosis (CMS32_DESC_LONG_DX.txt) and also Procedure (CMS32_DESC_LONG_SG.txt)  Codes.  The ICD-9 Diagnosis tables provide a description of the numerical Diagnosis codes contained in the Challenge_1_Training.Set.csv file.  You can use this file if you want to understand the codes and/or deepen your analysis of re-admission causes.

**Data Download Links:** // Address to host the dataset
* Training Set:
* READ_ME.doc
* Challenge_1_Training.Set.csv
* admission_type_id.csv
* discharge_disposition_id.csv
* admission_source_id.csv

##### Challenge #2: To be updated by Zach

##### Challenge #3: Exploratory Data Analysis and Predictions.
If you're new to data science and machine learning, or looking for a simple intro to the Data Science this challenge will help you get started. There are three different datasets to chose from according to your interest. The main objective is to explore and understand data and answers simple questions such as :
- What are the general trends?
- Correlation analysis?
- Generate Predictions?

**Dataset 1:** 2016 Election Polls (https://www.kaggle.com/five-thirty-eight/2016-election-polls)

Understand the data and answer questions such as: 
- What are the trends of the polls over time (by day/week/month)?
- How do the trends vary by state?
- What are common attributes of voters by State.
- What is the probability that Trump/Clinton will win the 2016 election?


In this challenge, your team will use clustering methods and network science methods to identify â€œteamsâ€ of Medicare providers linked to each other by shared patients in the state of New York State (NYS).  You will use a subset of a publicly available Medicare data sets that contain only NYS providers (doctors, organizations, nurse practitioners, physicianâ€™s assistants, etc.).  You will need to link this data set to another file that contains information about each provider specialty as well as, a table containing geolocation coordinates of the provider at the zip code level.  You should identify closely linked teams of providers, and figure out their characteristics (team composition, etc.) and any other interesting features of how they are connected.  To accomplish this challenge, you will also need to display your results using innovative graphics. The team with the most elegant methods and visualizations will win a $100 prize.



Final Presentation Template:
The final presentation format is a 1 + 5 PowerPoint format template. Only the first 5 data slides will be included in the judging.


Judging Criteria:
 Teams will be judged based on three criteria: (1) Prediction â€“ How close your prediction came to the true readmission values, along with false positive and false negative rates.  You want to have a low false negative rate. (2) Presentation  (3) Deep Magic â€“ did you use 







plementing Feature 2 and 3).

`unverified` means the two users have not previously been involved in a transaction (when implementing Feature 1) or are outside of the "friends network" (when implementing Features 2 and 3)

The output is written to a text file in the `paymo_output` directory. Because we are asked to implement a minimum of three features, your program should output to at least three text files in the `paymo_output` directory. 

Each output file is named after the applicable feature you implemented (i.e., `output1.txt`, `output2.txt` and `output3.txt`)

For example, if there were 5 lines of transactions in the `stream_payment.csv`, then the following `output1.txt` file for Feature 1 could look like this: 

	trusted
	trusted
	unverified
	unverified
	trusted

An additional file `output4.txt` has been generated for the additional features created. Each payment, output a line containing one of two words, trusted or unverified, but in addition if record is unverified using additional features it will also have reasons why payment was classified as `unverified`. For example -
	
	Unverified 	 Reason: Payment 18.84 was suspicious, between users 4435 and 377 at row 2666
	Unverified 	 Reason: Payment 98.74 has exceeded maximum payment, between users 1 and 8
    Unverified 	 Reason: Payment 23.74 has expired, between users 1 and 16
	
## Description of Data

[Back to Table of Contents] (README.md#table-of-contents)

The `batch_payment.csv` and `stream_payment.csv` input files are formatted the same way.

As you would expect of comma-separated-value files, the first line is the header. It contains the names of all of the fields in the payment record. In this case, the fields are 

* `time`: Timestamp for the payment 
* `id1`: ID of the user making the payment 
* `id2`: ID of the user receiving the payment 
* `amount`: Amount of the payment 
* `message`: Any message the payer wants to associate with the transaction

Following the header, you can assume each new line contains a single new PayMo payment record with each field delimited by a comma. In some cases, the field can contain Unicode as PayMo users are fond of placing emojis in their messages. For simplicity's sake, you can choose to ignore those emojis.

For example, the first 10 lines (including the header) of `batch_payment.csv` or `stream_payment.csv` could look like: 

	time, id1, id2, amount, message
	2016-11-02 09:49:29, 52575, 1120, 25.32, Spam
	2016-11-02 09:49:29, 47424, 5995, 19.45, Food for ğŸŒ½ ğŸ˜
	2016-11-02 09:49:29, 76352, 64866, 14.99, Clothing
	2016-11-02 09:49:29, 20449, 1552, 13.48, LoveWins
	2016-11-02 09:49:29, 28505, 45177, 19.01, ğŸŒğŸ»ğŸŒ²ğŸ”ğŸ†
	2016-11-02 09:49:29, 56157, 16725, 4.85, 5
	2016-11-02 09:49:29, 25036, 24692, 20.42, Electric
	2016-11-02 09:49:29, 70230, 59830, 19.33, Kale Salad
	2016-11-02 09:49:29, 63967, 3197, 38.09, Diner
	 

## Writing clean, scalable and well-tested code
[Back to Table of Contents] (README.md#table-of-contents)

As a data engineer, itâ€™s important you write clean, well-documented code that scales for large amounts of data. For this reason, itâ€™s important to ensure your solution works well for a huge number of payments coming in a short period of time.

It's also important to use software engineering best practices like **unit tests**, especially since public data is not clean and predictable. For more details about the implementation, please refer to the FAQ below or email us at <mailto:cc@insightdataengineering.com>

You may write your solution in any mainstream programming language, such as C, C++, Java, JavaScript, Python, Ruby, or Scala. Once completed, submit a link to a Github (or Bitbucket) repo with your source code. 

In addition to the source code, the top-most directory of your repo must include the `paymo_input` and `paymo_output` directories, and a shell script named `run.sh` that compiles and runs the program(s) that implement these features. 

If your solution requires additional libraries, environments, or dependencies, you must specify these in your README documentation (otherwise we won't be able to test it). See the figure below for the required structure of the top-most directory in your repo, or simply <i>clone</i> this repo, but **please don't fork** it.

### Implementation Details

**Language :** Python

**Libraries :** following python libraries we called - csv, time, collection

**System Specification :** The solution was run on 

1. Unix(macOS Sierra v10.12.1,Intel(R) Core(TM) i7-5557U CPU @ 3.10GHz)
2. Linux(Ubuntu 15.10, Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz)

**High Level Software Design :**
This program has been divided in four different stages. Starts with reading batch file to build the graph, then reads stream file and test both core and additional features and filally write the results to output. Core features are mandatory requirements and have been implemented in `antifraud.py` . Whereas additional features have been programmed in `addedfeatures.py`Program is divided in following stages 

- Stage 1 - Batch Processing :** This stage reads a `batch_payments.txt` file, this file will be used to build social network of  digital wallet users based on their payments history. This stage also finds the maximum payable cap.
- Stage 2: Stream Processing:** This stage reads stream of incoming payments from `stream_payments.txt` that are being being made between two users. The challenge is to detect if the payment is TRUSTED or UNVERIFIED using three core features mentioned above.
- Stage 3: Each payment once classified will be written to the four output files. 
- Stage 4: This stage implements additional features to prevent any fraudulent payments. Each incoming payment is checked for additional features after Stage 2. These features have been written in `addedfeatures.py` .

**Testing :** 
Test cases have been written under `insight_testsuite/tests`. 
> NOTE: This Code has been successfully tested on linux platform for original `batch_payment.csv` and `stream_payment.csv` files (provided at the dropbox) but could not be included due to the size limitations.

- **Tests for Core Features**
- test-1-paymo-trans: This tests Feature 1. Unit test case, Degree 1 connections.
- test-2-paymo-trans: This tests Feature 2. Unit test case, Degree 1 and Degree 2 connections.
- test-3-paymo-trans: This tests Feature 3. Unit test case, Degree 1, 2, 3 and 4 connections.
- test-4-paymo-trans: This tests all failure cases. Unit test case.
- test-5-paymo-trans: This tests all the features. This include 15 distinct users.  
- test-6-paymo-trans: This tests all the Features including failure cases. This include more than 100 distinct users.


- **Tests for Additional Features**
- test-7-paymo-trans: This tests Feature 1. Unit test case, High volume. (Case: >10 connections in 1o sec).
- test-8-paymo-trans: This tests Feature 2. Unit test case, Expired payment. (3 days older payment)
- test-9-paymo-trans: This tests Feature 3. Unit test case, High Amount.
- test-10-paymo-trans: This tests all the additional features. This test case has 15 distinct users.
- test-11-paymo-trans: This test tests all the features including Core and Additional features. System Test.

**Result :**
Each transaction once identified will be written into four files. `Output1.txt`, `Output2.txt` and `Output3.txt` these files are named after each core feature implemented. `Output4.txt` will generate results for new features.
 
** Exception Handling :** Following exceptions have been taken care of : 
1. Invalid input Records (IndexError) : when record from `batch_payment` or `stream_payment` files is in incorrect format.
2. Invalid values of input fields (ValueError) : when field from `batch_payment` or `stream_payment` record has invalid value.

## Repo directory structure
[Back to Table of Contents] (README.md#table-of-contents)

Example Repo Structure

	â”œâ”€â”€ README.md 
	â”œâ”€â”€ run.sh
	â”œâ”€â”€ src
	â”‚  	â””â”€â”€ antifraud.py
	â”‚  	â””â”€â”€ addedfeatures.py
	â”œâ”€â”€ paymo_input
	â”‚   â””â”€â”€ batch_payment.csv
	|   â””â”€â”€ stream_payment.csv
	â”œâ”€â”€ paymo_output
	â”‚   â””â”€â”€ output1.txt
	|   â””â”€â”€ output2.txt
	|   â””â”€â”€ output3.txt
	|   â””â”€â”€ output4.txt
	â””â”€â”€ insight_testsuite
	 	   â”œâ”€â”€ run_tests.sh
		   â””â”€â”€ tests
	        	â””â”€â”€ test-1-paymo-trans
        		â”‚   â”œâ”€â”€ paymo_input
        		â”‚   â”‚   â””â”€â”€ batch_payment.csv
        		â”‚   â”‚   â””â”€â”€ stream_payment.csv
        		â”‚   â””â”€â”€ paymo_output
        		â”‚       â””â”€â”€ output1.txt
        		â”‚       â””â”€â”€ output2.txt
        		â”‚       â””â”€â”€ output3.txt
        		â”‚       â””â”€â”€ output4.txt
        		â””â”€â”€ your-own-test
            		 â”œâ”€â”€ paymo_input
        		     â”‚   â””â”€â”€ batch_payment.csv
        		     â”‚   â””â”€â”€ stream_payment.csv
        		     â””â”€â”€ paymo_output
        		         â””â”€â”€ output1.txt
        		         â””â”€â”€ output2.txt
        		         â””â”€â”€ output3.txt
        		         â””â”€â”€ output4.txt

Source directory `src` contains two files `antifraud.py` and `addedfeatures.py`. Output directory `paymo_output` contains four files.

## Testing your directory structure and output format
[Back to Table of Contents] (README.md#table-of-contents)

To make sure that your code has the correct directory structure and the format of the output data in `output1.txt`, `output2.txt` and `output3.txt` is correct, we included a test script, called `run_tests.sh` in the `insight_testsuite` folder.

The tests are stored simply as text files under the `insight_testsuite/tests` folder. Each test should have a separate folder and each should contain a `paymo_input` folder -- where `batch_payment.csv` and `stream_payment.csv` files can be found. There also should be a `paymo_output` folder where `output1.txt`, `output2.txt` and `output3.txt` should reside.

From the `insight_testsuite` folder, you can run the test with the following command:

	insight_testsuite$ ./run_tests.sh 

The output of `run_tests.sh` should look like:

    [PASS]: test-2-paymo-trans (output1.txt)
    [FAIL]: test-2-paymo-trans (output2.txt)
    1c1
    < trusted
    ---
    > unverified
    [PASS]: test-2-paymo-trans (output3.txt
    [Fri Nov  4 13:20:25 PDT 2016] 2 of 3 tests passed

on failed tests and	
	
	[PASS]: test-1-paymo-trans (output1.txt)
	[PASS]: test-1-paymo-trans (output2.txt)
	[PASS]: test-1-paymo-trans (output3.txt)
	[Fri Nov  4 13:20:25 PDT 2016] 3 of 3 tests passed
on success.

One test has been provided as a way to check your formatting and simulate how we will be running tests when you submit your solution. We urge you to write your own additional tests here as well as for your own programming language. `run_tests.sh` should alert you if the directory structure is incorrect.

Your submission must pass at least the provided test in order to pass the coding challenge.

#FAQ

Here are some common questions we've received.  If you have additional questions, please email us at cc@insightdataengineering.com and we'll answer your questions as quickly as we can, and update this FAQ.

* *Which Github link should I submit?*  
You should submit the URL for the top-level root of your repository.  For example, this repo would be submitted by copying the URL `https://github.com/InsightDataScience/digital-wallet` into the appropriate field on the application.  Do NOT try to submit your coding challenge using a pull request, which would make your source code publicly available.  

* *Do I need a private Github repo?*  
No, you may use a public repo, there is no need to purchase a private repo.  You may also submit a link to a Bitbucket repo if you prefer.

* *If User A sends a payment to User B, is that different than if User B sends a payment to User A?*  
No, for simplicity all relationships should be undirected. Users are "friends" regardless of who initiated the payment.  

* *May I use R, Matlab, or other analytics programming languages to solve the challenge?*  
It's important that your implementation scales to handle large amounts of data. While many of our Fellows have experience with R and Matlab, applicants have found that these languages are unable to process data in a scalable fashion, so you should consider another language.  

* *May I use distributed technologies like Hadoop or Spark?*  
While you're welcome to do so, your code will be tested on a single machine so there may not be a significant benefit to using these technologies prior to the program. With that said, learning about distributed systems is a valuable skill for all data engineers.

* *What sort of system should I use to run my program on (Windows, Linux, Mac)?*  
You may write your solution on any system, but your source code should be portable and work on all systems. Additionally, your `run.sh` must be able to run on either Unix or Linux, as that's the system that will be used for testing. Linux machines are the industry standard for most data engineering teams, so it is helpful to be familiar with this. If you're currently using Windows, we recommend using tools like Cygwin or Docker, or a free online IDE such as [Cloud9](http://c9.io).  
  
* *Can I use pre-built packages, modules, or libraries?*   
This coding challenge can be completed without any "exotic" packages. While you may use publicly available packages, modules, or libraries, you must document any dependencies in your accompanying `README` file. When we review your submission, we will download these libraries and attempt to run your program. If you do use a package, you should always ensure that the module you're using works efficiently for the specific use-case in the challenge, since many libraries are not designed for large amounts of data.

* *Can I use a database engine?*   
This coding challenge can be completed without the use of a database. However, if you must use one, it must be a publicly available one that can be easily installed with minimal configuration.

* *Will you email me if my code doesn't run?*   
Unfortunately, we receive hundreds of submissions in a very short time and are unable to email individuals if code doesn't compile or run. This is why it's so important to document any dependencies you have, as described in the previous question. We will do everything we can to properly test your code, but this requires good documentation. More so, we have provided a test suite so you can confirm that your directory structure and format are correct.

* *Do I need to use multi-threading?*   
No, your solution doesn't necessarily need to include multi-threading - there are many solutions that don't require multiple threads/cores or any distributed systems, but instead use efficient data structures.  

* *Do I need to account for an updating `stream_payment.csv` file?*   
No, your solution doesn't have to re-process `stream_payment.csv` as if it were updating in real-time. If you were doing this project as a data engineer in industry, you would probably connect to a RESTful API to get one transaction at a time, but this is beyond the scope of this challenge. Instead, you should imagine that each line corresponds to a new sequential transaction. 

* *What should the format of the output be?*  
In order to be tested correctly, you must use the format described above. You can ensure that you have the correct format by using the testing suite we've included. If you are still unable to get the correct format from the debugging messages in the suite, please email us at cc@insightdataengineering.com.

* *Should I check if the files in the input directory are text files or non-text files(binary)?*  
No, for simplicity you may assume that all of the files in the input directory are text files, with the format as described above.

* *Can I use an IDE like Eclipse or IntelliJ to write my program?*  
Yes, you can use what ever tools you want -  as long as your `run.sh` script correctly runs the relevant target files and creates the `output1.txt`, `output2.txt`, `output3.txt` files in the `paymo_output` directory.

* *What should be in the `paymo_input` directory?*  
You can put any text file you want in the directory since our testing suite will replace it. Indeed, using your own input files would be quite useful for testing.

* *How will the coding challenge be evaluated?*  
Generally, we will evaluate your coding challenge with a testing suite that provides a variety of inputs and checks the corresponding output.  This suite will attempt to use your `run.sh` and is fairly tolerant to different runtime environments.  Of course, there are many aspects (e.g. clean code, documentation) that cannot be tested by our suite, so each submission will also be reviewed manually by a data engineer. 

* *How long will it take for me to hear back from you about my submission?*  
We receive hundreds of submissions and try to evaluate them all in a timely manner.  We try to get back to all applicants **within two or three weeks of submission**, but if you have a specific deadline that requires expedited review, you may email us at cc@insightdataengineering.com.  

